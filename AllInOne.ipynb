{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP41bIBTuhaBNjdapZFiOvL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshataKurane/Shark-Tank/blob/main/AllInOne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages to installed"
      ],
      "metadata": {
        "id": "O7sFId2684Id"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xwcABUxRzz45",
        "outputId": "eebe6764-23c8-4492-9299-abb3cdfec697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-vnu2418s\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-vnu2418s\n",
            "  Resolved https://github.com/openai/whisper.git to commit 279133e3107392276dc509148da1f41bfb532c7e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.7.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20231117) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (1.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n",
            "Requirement already satisfied: pocketsphinx in /usr/local/lib/python3.10/dist-packages (5.0.3)\n",
            "Requirement already satisfied: sounddevice in /usr/local/lib/python3.10/dist-packages (from pocketsphinx) (0.5.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice->pocketsphinx) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice->pocketsphinx) (2.22)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.10)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.10 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.24.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install ffmpeg\n",
        "!pip install pydub\n",
        "!pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip\n",
        "!pip install -qq ipython==7.34.0\n",
        "!pip install SpeechRecognition\n",
        "!pip install pocketsphinx\n",
        "!pip install PyMuPDF sentence-transformers faiss-cpu transformers reportlab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcription with user input for path"
      ],
      "metadata": {
        "id": "7lDxt43g7IA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from pydub import AudioSegment\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def convert_mp3_to_wav(mp3_file_path, wav_file_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
        "    audio.export(wav_file_path, format=\"wav\")\n",
        "\n",
        "def rttm_to_dataframe(rttm_file_path):\n",
        "    columns = [\"Type\", \"File ID\", \"Channel\", \"Start Time\", \"Duration\", \"Orthography\", \"Confidence\", \"Speaker\", 'x', 'y']\n",
        "    with open(rttm_file_path, 'r') as rttm_file:\n",
        "        lines = rttm_file.readlines()\n",
        "        data = [line.strip().split() for line in lines]\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    df = df.drop([\"Type\", \"File ID\", \"Channel\", \"Orthography\", \"Confidence\", 'x', 'y'], axis=1)\n",
        "    return df\n",
        "\n",
        "def extract_text_from_audio_segment(audio_segment):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_file:\n",
        "        temp_path = temp_file.name\n",
        "        audio_segment.export(temp_path, format=\"wav\")\n",
        "        result = model.transcribe(temp_path)\n",
        "        os.remove(temp_path)\n",
        "    return result['text']\n",
        "\n",
        "def get_audio_segment(audio_file_path, start_time, end_time):\n",
        "    audio = AudioSegment.from_wav(audio_file_path)\n",
        "    start_ms = int(start_time * 1000)\n",
        "    end_ms = int(end_time * 1000)\n",
        "    return audio[start_ms:end_ms]\n",
        "\n",
        "def process_audio_file(mp3_file_path):\n",
        "    # Convert MP3 to WAV\n",
        "    wav_file_path = mp3_file_path.replace('.mp3', '.wav')\n",
        "    convert_mp3_to_wav(mp3_file_path, wav_file_path)\n",
        "\n",
        "    # Speaker diarization\n",
        "    pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=\"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\")\n",
        "    diarization = pipeline(wav_file_path, num_speakers=2)\n",
        "\n",
        "    rttm_file_path = wav_file_path.replace('.wav', '.rttm')\n",
        "    with open(rttm_file_path, \"w\") as rttm:\n",
        "        diarization.write_rttm(rttm)\n",
        "\n",
        "    # Process RTTM to DataFrame\n",
        "    df = rttm_to_dataframe(rttm_file_path)\n",
        "    df = df.astype({'Start Time': 'float', 'Duration': 'float'})\n",
        "    df['Utterance'] = None\n",
        "    df['End Time'] = df['Start Time'] + df['Duration']\n",
        "\n",
        "    # Transcribe audio segments\n",
        "    for ind in df.index:\n",
        "        start_time = df.loc[ind, 'Start Time']\n",
        "        end_time = df.loc[ind, 'End Time']\n",
        "        try:\n",
        "            audio_segment = get_audio_segment(wav_file_path, start_time, end_time)\n",
        "            transcription = extract_text_from_audio_segment(audio_segment)\n",
        "            df.loc[ind, 'Utterance'] = transcription\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing index {ind}: {e}\")\n",
        "            df.loc[ind, 'Utterance'] = 'Error'\n",
        "    print(df)\n",
        "\n",
        "    # Save DataFrame to CSV\n",
        "    output_csv_path = mp3_file_path.replace('.mp3', '.csv')\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    #print(f\"Processed {mp3_file_path}. Results saved to {output_csv_path}\")\n",
        "\n",
        "    file_format = input(\"Your output is saved in .csv format. Do you want in any other format? (e.g. csv, text, excel) : \")\n",
        "    if file_format == \"text\":\n",
        "      output_csv_path = mp3_file_path.replace('.mp3', '.txt')\n",
        "      df.to_csv(output_csv_path, index=False)\n",
        "    elif file_format == \"excel\":\n",
        "      output_csv_path = mp3_file_path.replace('.mp3', '.xlsx')\n",
        "      df.to_excel(output_csv_path, index=False)\n",
        "    else:\n",
        "      print(\"\\nInvalid file format\")\n",
        "    print(f\"\\nProcessed {mp3_file_path}. Results saved to {output_csv_path}\")\n",
        "\n",
        "mp3_file_path = input(\"Enter the path of audio file : \")\n",
        "process_audio_file(mp3_file_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndgfuUmt0bta",
        "outputId": "4bb17337-74b9-4411-9eb9-9bd1484fdcbb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path of audio file : /content/Shark Tank/videoplaybackshort.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.4.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "   Start Time  Duration     Speaker  \\\n",
            "0       0.031    13.298  SPEAKER_01   \n",
            "1      13.328     3.324  SPEAKER_00   \n",
            "2      16.653     3.679  SPEAKER_01   \n",
            "3      19.471     8.387  SPEAKER_00   \n",
            "4      28.482    63.990  SPEAKER_00   \n",
            "\n",
            "                                           Utterance  End Time  \n",
            "0   One thing which I've taken away for the wealt...    13.329  \n",
            "1   People are running out of money because of ba...    16.652  \n",
            "2   No, they're in fact, they will live so long. ...    20.332  \n",
            "3   What do you think Adani and Mukesham money do...    27.858  \n",
            "4   Now there's countless content pieces on YouTu...    92.472  \n",
            "Your output is saved in .csv format. Do you want in any other format? (e.g. csv, text, excel) : text\n",
            "\n",
            "Processed /content/Shark Tank/videoplaybackshort.mp3. Results saved to /content/Shark Tank/videoplaybackshort.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "ZGD9mgDclWGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "import re\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Frame\n",
        "\n",
        "# Load the pre-trained models\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "qa_model = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "summarization_model = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "# Function to extract text from CSV\n",
        "def extract_text_from_csv(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    text = \"\"\n",
        "    for col in df.columns:\n",
        "        text += \" \".join(df[col].astype(str).tolist()) + \" \"\n",
        "    return text\n",
        "\n",
        "# Function to save summary to a PDF file with better formatting\n",
        "def save_summary_to_pdf(summary, pdf_path):\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    story = []\n",
        "    story.append(Paragraph(\"Summary of the Content:\", styles['Title']))\n",
        "    story.append(Paragraph(summary, styles['BodyText']))\n",
        "    doc.build(story)\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_path = \"/content/Shark Tank/videoplaybackshort.csv\"\n",
        "\n",
        "# Extract text from the CSV\n",
        "document_text = extract_text_from_csv(csv_path)\n",
        "\n",
        "# Split the document into sentences or paragraphs\n",
        "document_sentences = document_text.split('.')\n",
        "\n",
        "# Encode the document sentences\n",
        "document_embeddings = embedding_model.encode(document_sentences)\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = document_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(document_embeddings))\n",
        "\n",
        "# Function to search the document\n",
        "def search_document(query, top_k=5):\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    D, I = index.search(np.array(query_embedding), k=top_k)  # k is the number of nearest neighbors\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        results.append(document_sentences[idx])\n",
        "    return results\n",
        "\n",
        "# Function to get answers from the document using the QA model\n",
        "def get_answers(query, context):\n",
        "    return qa_model(question=query, context=context)\n",
        "\n",
        "# Function to summarize the document\n",
        "def summarize_text(text, max_length=150):\n",
        "    return summarization_model(text, max_length=max_length, min_length=30, do_sample=False)[0]['summary_text']\n",
        "\n",
        "summary = summarize_text(document_text)\n",
        "\n",
        "# Save the summary to a PDF file\n",
        "pdf_path = \"/content/Shark Tank/Summary.pdf\"\n",
        "save_summary_to_pdf(summary, pdf_path)\n",
        "\n",
        "print(\"\\nSummary of the Content:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIcyfXGxmtkv",
        "outputId": "5eae15a5-016b-4528-8c89-c8c0dba0a79a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of the Content:\n",
            " Sandeep Jaitwani is the founder of Dizerve.in, a modern day portfolio management service . He is handling some of beer by himself media world's funds . He will talk about how the richest in India manages their money and how you can manage your money .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "KCjDQ14MvHD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "pdf_path = \"/content/Shark Tank/Summary.pdf\"\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "print(pdf_text)\n",
        "\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiments = {\"positive\": [], \"negative\": [], \"neutral\": []}\n",
        "    for sentence in blob.sentences:\n",
        "        sentiment = sentence.sentiment.polarity\n",
        "        if sentiment > 0:\n",
        "            sentiments[\"positive\"].append((sentence, sentiment))\n",
        "        elif sentiment < 0:\n",
        "            sentiments[\"negative\"].append((sentence, sentiment))\n",
        "        else:\n",
        "            sentiments[\"neutral\"].append((sentence, sentiment))\n",
        "    return sentiments\n",
        "\n",
        "sentiments = analyze_sentiment(pdf_text)\n",
        "print(\"Positive Sentences:\", sentiments[\"positive\"])\n",
        "print(\"Negative Sentences:\", sentiments[\"negative\"])\n",
        "print(\"Neutral Sentences:\", sentiments[\"neutral\"])\n",
        "\n",
        "\n",
        "def overall_sentiment(sentiments):\n",
        "    total_sentences = len(sentiments[\"positive\"]) + len(sentiments[\"negative\"]) + len(sentiments[\"neutral\"])\n",
        "    overall_score = sum([score for _, score in sentiments[\"positive\"]]) + sum([score for _, score in sentiments[\"negative\"]]) + sum([score for _, score in sentiments[\"neutral\"]])\n",
        "    return overall_score / total_sentences if total_sentences != 0 else 0\n",
        "\n",
        "overall_score = overall_sentiment(sentiments)\n",
        "\n",
        "if overall_score > 0:\n",
        "    sentiment_label = \"Positive\"\n",
        "elif overall_score < 0:\n",
        "    sentiment_label = \"Negative\"\n",
        "else:\n",
        "    sentiment_label = \"Neutral\"\n",
        "\n",
        "print(\"Overall Sentiment Score:\", overall_score)\n",
        "print(\"Sentiment Label:\", sentiment_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OXsFHVUvEpU",
        "outputId": "73c38e29-37a1-48b4-f227-fec3c2e0f94b",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Summary of the Content:\n",
            "Sandeep Jaitwani is the founder of Dizerve.in, a modern day portfolio management service . He is\n",
            "handling some of beer by himself media world's funds . He will talk about how the richest in India\n",
            "manages their money and how you can manage your money .\n",
            "\n",
            "Positive Sentences: [(Sentence(\"Summary of the Content:\n",
            "Sandeep Jaitwani is the founder of Dizerve.in, a modern day portfolio management service .\"), 0.2)]\n",
            "Negative Sentences: []\n",
            "Neutral Sentences: [(Sentence(\"He is\n",
            "handling some of beer by himself media world's funds .\"), 0.0), (Sentence(\"He will talk about how the richest in India\n",
            "manages their money and how you can manage your money .\"), 0.0)]\n",
            "Overall Sentiment Score: 0.06666666666666667\n",
            "Sentiment Label: Positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation"
      ],
      "metadata": {
        "id": "pzQjN-DmA0X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PdfReader translate PyPDF2\n",
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "xQyjpr3dBbE9",
        "outputId": "041d2b13-cb52-4f15-91c6-9758eb7cc7e4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PdfReader\n",
            "  Downloading pdfreader-0.1.15-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting bitarray>=1.1.0 (from PdfReader)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from PdfReader) (10.4.0)\n",
            "Collecting pycryptodome>=3.9.9 (from PdfReader)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from PdfReader) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from translate) (8.1.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from translate) (4.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from translate) (2.32.3)\n",
            "Collecting libretranslatepy==2.1.1 (from translate)\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl.metadata (233 bytes)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->PdfReader) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2024.8.30)\n",
            "Downloading pdfreader-0.1.15-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libretranslatepy, bitarray, PyPDF2, pycryptodome, translate, PdfReader\n",
            "Successfully installed PdfReader-0.1.15 PyPDF2-3.0.1 bitarray-2.9.2 libretranslatepy-2.1.1 pycryptodome-3.20.0 translate-3.6.1\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.9.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.9.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=dd1bee0fb0cf6727e6551fa634028c457ab74c71c1c75155cf280b21739ad4e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.9.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "6360c59a8d9f499393fb1bf205d97502"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "\n",
        "def translate_csv(input_file, output_file, target_language):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Initialize the translator\n",
        "    translator = Translator()\n",
        "\n",
        "    # Create a new DataFrame for the translated text\n",
        "    translated_df = pd.DataFrame()\n",
        "\n",
        "    # Translate each column\n",
        "    for column in df.columns:\n",
        "        translated_column = df[column].apply(lambda x: translator.translate(str(x), dest=target_language).text)\n",
        "        translated_df[column] = translated_column\n",
        "\n",
        "    # Save the translated DataFrame to a new CSV file\n",
        "    translated_df.to_csv(output_file, index=False)\n",
        "\n",
        "# User input for file names and language\n",
        "input_file = input(\"Enter the input CSV file path: \")\n",
        "output_file = '/content/Shark Tank/TranslatedCSVFile.csv'\n",
        "target_language = input(\"Enter the target language code (e.g., 'es' for Spanish, 'fr' for French): \")\n",
        "\n",
        "# Translate the CSV\n",
        "translate_csv(input_file, output_file, target_language)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13svRG3PA2zH",
        "outputId": "7719411d-190d-41af-d535-c06648560644"
      },
      "execution_count": 33,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the input CSV file path: /content/Shark Tank/videoplaybackshort.csv\n",
            "Enter the target language code (e.g., 'es' for Spanish, 'fr' for French): fr\n"
          ]
        }
      ]
    }
  ]
}